
# Hands-on Lab - Day 3

### Tools for this Lab
* Azure Command-Line Interface (Az CLI) on Bash.



## Exercise 1 - Scale your application to meet demand
As the popularity of a website grows, the application needs to scale appropriately to manage demand changes. The goal is to ensure that applications remains responsive as the number of requests increases.

#### Task 1 - Create the horizontal pod autoscaler (HPA)

Create a file called ratings-api-hpa.yaml
```
vi ratings-api-hpa.yaml
```

Paste the following text in the file
```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: ratings-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ratings-api
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 30
```

Run the kubectl apply command to deploy th HPA object
```
kubectl apply \
    --namespace ratingsapp \
    -f ratings-api-hpa.yaml
```

#### Task 2 - Run a load test with horizontal pod autoscaler enabled
To create the load test, you can use a prebuilt image called azch/artillery. The image contains a tool calle artillery that is used to send traffic to the API. For this lab you will use Azure Container Instances to run this image as a container.

Get the ingress endpoint (column `HOSTS`)
```
kubectl get ingress -n ratingsapp
```

Assign the value in the hosts column to an environment variable
```
LOADTEST_API_ENDPOINT="https://<put your front hostname here>/api/loadtest"
```

Run the load test with a duration of 120 seconds to simulate up to 700 requests per second
```
RESOURCE_GROUP="aksworkshop-RG"
az container create \
    -g $RESOURCE_GROUP \
    -n loadtest \
    --cpu 4 \
    --memory 1 \
    --image azch/artillery \
    --restart-policy Never \
    --command-line "artillery quick -r 700 -d 120 $LOADTEST_API_ENDPOINT"
```

#### Task 3 - Watch the horizontal pod autoscaler working
Check how the HPA identifies an increase in CPU usage and increases the replica set to provision more pods.  
```
kubectl get hpa -n ratingsapp -w 
```


Go to the Azure Portal and check the Insights section on you AKS Cluster.



## Exercise 2 - Autoscale the AKS cluster
HPA scales out with new pods as required. Eventually, the cluster runs out of resources, and you'll see scheduled pods in a pending state.

The cluster autoscaler watches for pods that can't be scheduled on nodes because of resource constraints. The cluster then automatically increases the number of nodes in the cluster.

To simulate that load force cluster to autoscale, we can artificially increase the resource `request` and `limit` for CPU. This forces the pods to request more resources across the cluster than is actually available. 

#### Task 1 - Increase pods requirements to force autoscale
Edit ratings API deployment manifest file
```
vi ratings-api-deployment.yaml
```

Change the `resources.requests` and `resources.limit` for the container to be 1000m, which means one core. That section should now look like this:
```yaml
resources:
  requests: # minimum resources required
    cpu: 1000m
    memory: 64Mi
  limits: # maximum resources allocated
    cpu: 1000m
    memory: 256Mi
```

Apply the new configuration.
```
kubectl apply \
    --namespace ratingsapp \
    -f ratings-api-deployment.yaml
```

#### Task 2 - Check that cluster is not able to create new pods
After changing configuration multiple pods get stuck in the `Pending` state because there isn't enough capacity on the cluster to schedule new pods
```
kubectl get pods -n ratingsapp
```

#### Task 3 - Configure the cluster autoscaler
Check your the random number you have used in previous labs and assign it to the variable NUMBER
```
NUMBER=####
```

Define environment variables
```
AKS_NAME="k8s-cluster"$NUMBER
RESOURCE_GROUP="aksworkshop-RG"
```

Configure the cluster autoscaler
``` 
az aks update \
    --resource-group $RESOURCE_GROUP \
    --name $AKS_NAME  \
    --enable-cluster-autoscaler \
    --min-count 3 \
    --max-count 5
```

#### Task 4 - Verify the number of nodes has increased
In a few minutes, new nodes will pop up and transition to the `Ready` state.
```
kubectl get nodes -w
```

The increase in capacity will allow pending pods to transition to `Ready` state too.
```
kubectl get pods -n ratingsapp
```

## Exercise 3 - Creating a Continous Integration pipeline with GitHub Actions

#### Task 1 - Setup at GitHub

* Login into [GitHub.com](https://github.com). If you don't have a GitHub account create one by clicking the `Sign up` button at the right top corner.

* Fork [the sample repository](https://github.com/carlosalexei/ratings-web). Navigate to the repository and click the `Fork` button at the right top corner.


#### Task 2 - Create a GitHub action

* Go to your forked repository and select the `Actions` tab from the menu at the top
* Click `set up a workflow yourself`
* A new file will be created under `.github/workflows`. Change the name to `build-image.yml`
* Replace the content of the file with the following text
```yml
name: Build and push the latest build to ACR

on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build_push_image:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - uses: actions/checkout@v2
        
      - name: Set up Buildx
        uses: docker/setup-buildx-action@v1    
      
      - name: Docker Login
        uses: docker/login-action@v1.10.0
        with:
          registry: ${{ secrets.ACR_NAME }}
          username: ${{ secrets.ACR_LOGIN }}
          password: ${{ secrets.ACR_PASSWORD }}
          
      - name: Build and push staging images
        uses: docker/build-push-action@v2
        with:
          context: .
          tags: ${{secrets.ACR_NAME}}/ratings-web:latest
          push: true
```
* To commit the changes, select the green `Start commit` button. Enter a description for the commit and then select the `Commit new file`


#### Task 3 - Set secrets
Asign the number you have use previously to the NUMBER variable
```
NUMBER=####
```

Define the following variables
```
$REGISTRY='containerregistry'$NUMBER
```


* On the repository start page, select the `Settings` tab. In the menu, select `Secrets`
* For each of the following secrets use the corresponding command to get the right value:
<table>
    <thead>
        <tr>
            <td>Secret</tb>
            <td>Execute the following command to get the value</tb>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>ACR_NAME</td>
            <td>az acr list --query "[?contains(resourceGroup, 'aksworkshop-RG')].loginServer" -o table</td>
        </tr>
        <tr>
            <td>ACR_LOGIN</td>
            <td>az acr credential show --name $REGISTRY --query "username" -o table</td>
        </tr>
        <tr>
            <td>ACR_PASSWORD</td>
            <td>az acr credential show --name $REGISTRY --query "passwords[0].value" -o table</td>
        </tr>
    </tbody>
</table>

#### Task 4 - Push the image
* Select the `Actions` tab
* Select the only execution in the list
* On the right side, select `Re-run all jobs`

#### Task 5 - Confirm that the repository now includes the latest version
```
az acr repository list --name $REGISTRY -o table
```




## Clean up
At the end of the workshop you might want to clean up all resources so that there's no continued charge against your account for these resources

Alternatively, you can stop your AKS Cluster for future use. 
```
az aks stop -n $AKS_NAME -g $RESOURCE_GROUP
```


